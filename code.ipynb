{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project - Natural Language Processing and Text Classification\n",
    "\n",
    "This project makes use of NLP techniques to train models to predict different classes of angry comments. Data containing a group of different texts are then fed to the models to generate a prediction classifying the texts into their respective category of angry. Membership in one output category is <b>independent</b> of membership in any of the other classes. Meaning a comment may belong to any number of categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preview the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                       comment_text  toxic  severe_toxic  \\\n",
       "0   1  Explanation\\nWhy the edits made under my usern...      0             0   \n",
       "1   2  D'aww! He matches this background colour I'm s...      0             0   \n",
       "2   3  Hey man, I'm really not trying to edit war. It...      0             0   \n",
       "3   4  \"\\nMore\\nI can't make any real suggestions on ...      0             0   \n",
       "4   5  You, sir, are my hero. Any chance you remember...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv.zip\")\n",
    "train_df = train_df.drop(columns='id')\n",
    "train_df.insert(0, 'ID', range(1, 1 + len(train_df)))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.99      0.97     36065\n",
      "           1       0.88      0.41      0.56      3828\n",
      "\n",
      "    accuracy                           0.94     39893\n",
      "   macro avg       0.91      0.70      0.76     39893\n",
      "weighted avg       0.93      0.94      0.93     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=500)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y = train_df[\"toxic\"]\n",
    "x = train_df[\"comment_text\"]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "\n",
    "pipe = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe.fit(x_train, y_train.ravel())\n",
    "preds = pipe.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severe Toxic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     39477\n",
      "           1       0.47      0.08      0.14       416\n",
      "\n",
      "    accuracy                           0.99     39893\n",
      "   macro avg       0.73      0.54      0.57     39893\n",
      "weighted avg       0.99      0.99      0.99     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=1000)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y1 = train_df[\"severe_toxic\"]\n",
    "x1 = train_df[\"comment_text\"]\n",
    "\n",
    "x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1)\n",
    "\n",
    "pipe1 = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe1.fit(x1_train, y1_train.ravel())\n",
    "preds1 = pipe1.predict(x1_test)\n",
    "\n",
    "print(classification_report(y1_test, preds1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obscene Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99     37756\n",
      "           1       0.90      0.63      0.74      2137\n",
      "\n",
      "    accuracy                           0.98     39893\n",
      "   macro avg       0.94      0.81      0.86     39893\n",
      "weighted avg       0.98      0.98      0.97     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=1000)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y2 = train_df[\"obscene\"]\n",
    "x2 = train_df[\"comment_text\"]\n",
    "\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2)\n",
    "\n",
    "pipe2 = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe2.fit(x2_train, y2_train.ravel())\n",
    "preds2 = pipe2.predict(x2_test)\n",
    "\n",
    "print(classification_report(y2_test, preds2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39772\n",
      "           1       0.94      0.14      0.24       121\n",
      "\n",
      "    accuracy                           1.00     39893\n",
      "   macro avg       0.97      0.57      0.62     39893\n",
      "weighted avg       1.00      1.00      1.00     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=1000)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y3 = train_df[\"threat\"]\n",
    "x3 = train_df[\"comment_text\"]\n",
    "\n",
    "x3_train, x3_test, y3_train, y3_test = train_test_split(x3, y3)\n",
    "\n",
    "pipe3 = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe3.fit(x3_train, y3_train.ravel())\n",
    "preds3 = pipe3.predict(x3_test)\n",
    "\n",
    "print(classification_report(y3_test, preds3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insult Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98     37961\n",
      "           1       0.75      0.23      0.35      1932\n",
      "\n",
      "    accuracy                           0.96     39893\n",
      "   macro avg       0.85      0.61      0.67     39893\n",
      "weighted avg       0.95      0.96      0.95     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=1000)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y4 = train_df[\"insult\"]\n",
    "x4 = train_df[\"comment_text\"]\n",
    "\n",
    "x4_train, x4_test, y4_train, y4_test = train_test_split(x4, y4)\n",
    "\n",
    "pipe4 = Pipeline([ \n",
    "                    (\"vect\", vec_cv),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe4.fit(x4_train, y4_train.ravel())\n",
    "preds4 = pipe4.predict(x4_test)\n",
    "\n",
    "print(classification_report(y4_test, preds4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity Hate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00     39529\n",
      "           1       0.80      0.15      0.25       364\n",
      "\n",
      "    accuracy                           0.99     39893\n",
      "   macro avg       0.90      0.57      0.62     39893\n",
      "weighted avg       0.99      0.99      0.99     39893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vec_tf = TfidfVectorizer(max_features=1000)\n",
    "vec_cv = CountVectorizer(max_features=150)\n",
    "\n",
    "y5 = train_df[\"identity_hate\"]\n",
    "x5 = train_df[\"comment_text\"]\n",
    "\n",
    "x5_train, x5_test, y5_train, y5_test = train_test_split(x5, y5)\n",
    "\n",
    "pipe5 = Pipeline([ \n",
    "                    (\"vect\", vec_tf),\n",
    "                    (\"model\", SVC())\n",
    "])\n",
    "\n",
    "params = [\"vec_cv\"]\n",
    "\n",
    "pipe5.fit(x5_train, y5_train.ravel())\n",
    "preds5 = pipe5.predict(x5_test)\n",
    "\n",
    "print(classification_report(y5_test, preds5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and preview the test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                       comment_text\n",
       "0   1  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1   2  == From RfC == \\n\\n The title is fine as it is...\n",
       "2   3  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3   4  :If you have a look back at the source, the in...\n",
       "4   5          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"test.csv\")\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save trained model reload and Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe, \"toxic.joblib\")\n",
    "premade_model = load(\"toxic.joblib\")\n",
    "toxic_preds = premade_model.predict(test_df[\"comment_text\"])\n",
    "toxic_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Severe Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe1, \"severe_toxic.joblib\")\n",
    "premade_model1 = load(\"severe_toxic.joblib\")\n",
    "severetoxic_preds = premade_model1.predict(test_df[\"comment_text\"])\n",
    "severetoxic_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe2, \"obscene.joblib\")\n",
    "premade_model2 = load(\"obscene.joblib\")\n",
    "obscene_preds = premade_model2.predict(test_df[\"comment_text\"])\n",
    "obscene_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe3, \"threat.joblib\")\n",
    "premade_model3 = load(\"threat.joblib\")\n",
    "threat_preds = premade_model3.predict(test_df[\"comment_text\"])\n",
    "threat_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insult.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## saving the model for insult\n",
    "dump(pipe4, \"insult.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Loading and predicting\n",
    "premade_model4 = load(\"insult.joblib\")\n",
    "insult_preds = premade_model4.predict(test_df[\"comment_text\"])\n",
    "insult_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identity Hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(pipe5, \"identity_hate.joblib\")\n",
    "premade_model5 = load(\"identity_hate.joblib\")\n",
    "identityhate_preds = premade_model5.predict(test_df[\"comment_text\"])\n",
    "identityhate_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Details\n",
    "Predictions are loaded to a CSV file. With a row per test data and columns for each classification. The presence of a (1) in a column indicates that the comment on that row belongs to that class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coverting the index column of the data frame to a numpy array\n",
    "id = test_df.index.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0   0      1             0        1       0       0              0\n",
       "1   1      0             0        0       0       0              0\n",
       "2   2      0             0        0       0       0              0\n",
       "3   3      0             0        0       0       0              0\n",
       "4   4      0             0        0       0       0              0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct dummy data for a sample output. \n",
    "#You won't do this part, you have real data\n",
    "#Your data should have the same structure, so the CSV output is the same\n",
    "columns = [\"id\", \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "sample_out = pd.DataFrame( list(zip(id, toxic_preds, severetoxic_preds, obscene_preds, threat_preds, insult_preds, identityhate_preds)),\n",
    "                    columns=columns)\n",
    "sample_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write DF to CSV. Please keep the \"out.csv\" filename. Moodle will auto-preface it with an identifier when I download it. \n",
    "#This command should work with your dataframe of predictions. \n",
    "sample_out.to_csv('out.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d722d3adfa415172c1f5238b519fb86b488acdae450fd691ab06c09f4ca9173"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('ml3950': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
